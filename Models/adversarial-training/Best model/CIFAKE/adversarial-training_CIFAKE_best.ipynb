{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import o\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_vgg19():\n",
    "    base = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "    x = Flatten()(base.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(base.input, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\n",
    "val_dir = \"/kaggle/working/validation\"\n",
    "\n",
    "for label in ['REAL', 'FAKE']:\n",
    "    os.makedirs(os.path.join(val_dir, label), exist_ok=True)\n",
    "    files = os.listdir(os.path.join(base_dir, label))\n",
    "    train_files, val_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "\n",
    "    for fname in val_files:\n",
    "        src = os.path.join(base_dir, label, fname)\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train',\n",
    "    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=True)\n",
    "\n",
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    '/kaggle/working/validation',\n",
    "    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=False)\n",
    "\n",
    "test_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test',\n",
    "    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM Attack Function and PGD Attack Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def fgsm_attack(model, images, labels, epsilon=0.05):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        predictions = model(images)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()(labels, predictions)\n",
    "    gradient = tape.gradient(loss, images)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    adv_images = images + epsilon * signed_grad\n",
    "    return tf.clip_by_value(adv_images, 0, 1)\n",
    "\n",
    "# ✅ PGD Attack Function\n",
    "@tf.function\n",
    "def pgd_attack(x, y, model, loss_fn, epsilon=0.03, alpha=0.007, iters=10):\n",
    "    x_adv = tf.identity(x)\n",
    "\n",
    "    for i in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            prediction = model(x_adv, training=False)\n",
    "            loss = loss_fn(y, prediction)\n",
    "\n",
    "        grad = tape.gradient(loss, x_adv)\n",
    "        signed_grad = tf.sign(grad)\n",
    "        x_adv = x_adv + alpha * signed_grad\n",
    "        x_adv = tf.clip_by_value(x_adv, x - epsilon, x + epsilon)\n",
    "        x_adv = tf.clip_by_value(x_adv, 0.0, 1.0)  # ensure valid pixel range\n",
    "\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train and Save Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:23:59.637779Z",
     "iopub.status.busy": "2025-04-15T14:23:59.637526Z",
     "iopub.status.idle": "2025-04-15T14:26:55.280318Z",
     "shell.execute_reply": "2025-04-15T14:26:55.279689Z",
     "shell.execute_reply.started": "2025-04-15T14:23:59.637757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744727042.531207      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1744727042.531875      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Baseline Model on Clean Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744727047.967304     113 service.cc:148] XLA service 0x79de8c00c100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744727047.969142     113 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1744727047.969163     113 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1744727048.219918     113 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11:37\u001b[0m 13s/step - accuracy: 0.9375 - loss: 0.3287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744727059.586375     113 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 249ms/step - accuracy: 0.8809 - loss: 0.2490\n",
      "Baseline Clean Accuracy: [0.20891635119915009, 0.913349986076355]\n"
     ]
    }
   ],
   "source": [
    "# Paths to save model and weights\n",
    "weights_path_baseline = '/kaggle/working/vgg19_baseline_cifake.weights.h5'\n",
    "model_path_baseline   = '/kaggle/working/vgg19_baseline_cifake_model.h5'\n",
    "#Kaggle\n",
    "if os.path.exists('/kaggle/input/vgg19_baseline_cifake/tensorflow2/default/1/vgg19_baseline_cifake_model.h5'):\n",
    "    print(\"Loading saved model...\")\n",
    "    baseline_model = load_model('/kaggle/input/vgg19_baseline_cifake/tensorflow2/default/1/vgg19_baseline_cifake_model.h5')\n",
    "#Colab\n",
    "# if os.path.exists(model_path_baseline):\n",
    "#     print(\"Loading saved model...\")\n",
    "#     baseline_model = load_model(model_path_baseline)\n",
    "else:\n",
    "    print(\"Training baseline model...\")\n",
    "    baseline_model = build_vgg19()\n",
    "    baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    baseline_model.fit(train_gen, epochs=3, validation_data=val_gen)\n",
    "    baseline_model.save_weights(weights_path_baseline)\n",
    "    baseline_model.save(model_path_baseline)\n",
    "    print(\"Baseline model trained and saved.\")\n",
    "\n",
    "print(\"\\nEvaluating Baseline Model on Clean Test Set:\")\n",
    "baseline_clean_acc = baseline_model.evaluate(test_gen)\n",
    "print(\"Baseline Clean Accuracy:\", baseline_clean_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM Vulnerability Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:26:55.281274Z",
     "iopub.status.busy": "2025-04-15T14:26:55.281015Z",
     "iopub.status.idle": "2025-04-15T14:27:04.791783Z",
     "shell.execute_reply": "2025-04-15T14:27:04.791159Z",
     "shell.execute_reply.started": "2025-04-15T14:26:55.281255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Baseline Model on FGSM-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.1250 - loss: 1.1523\n",
      "Baseline Accuracy on FGSM-Adversarial: [1.152273416519165, 0.125]\n"
     ]
    }
   ],
   "source": [
    "x_test_sample, y_test_sample = next(test_gen)\n",
    "x_test_tensor = tf.convert_to_tensor(x_test_sample, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_sample, dtype=tf.float32)\n",
    "\n",
    "x_fgsm_test = fgsm_attack(baseline_model, x_test_tensor, y_test_tensor)\n",
    "print(\"\\nEvaluating Baseline Model on FGSM-Adversarial Examples:\")\n",
    "baseline_fgsm_acc = baseline_model.evaluate(x_fgsm_test, y_test_sample)\n",
    "print(\"Baseline Accuracy on FGSM-Adversarial:\", baseline_fgsm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Save Perturbed Data (FGSM + PGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:28:17.783769Z",
     "iopub.status.busy": "2025-04-15T14:28:17.783200Z",
     "iopub.status.idle": "2025-04-15T16:43:30.293547Z",
     "shell.execute_reply": "2025-04-15T16:43:30.292772Z",
     "shell.execute_reply.started": "2025-04-15T14:28:17.783745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [2:14:12<00:00,  2.58s/it]  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directories\n",
    "base_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\n",
    "perturbed_dir = \"/kaggle/working/train_perturbed_only\"  # All perturbed, no clean data\n",
    "weights_path_baseline = \"/kaggle/input/vgg19_baseline_cifake/tensorflow2/default/1/vgg19_baseline_cifake.weights.h5\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(os.path.join(perturbed_dir, 'REAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(perturbed_dir, 'FAKE'), exist_ok=True)\n",
    "\n",
    "# Load baseline model (for generating perturbations)\n",
    "baseline_model = build_vgg19()\n",
    "baseline_model.load_weights(weights_path_baseline)  # Pretrained weights\n",
    "\n",
    "# Data generator (no shuffling for consistent file mapping)\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    base_train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False  # Critical for matching files to paths\n",
    ")\n",
    "\n",
    "# Attack parameters\n",
    "epsilon_fgsm = 0.05\n",
    "epsilon_pgd = 0.03\n",
    "alpha_pgd = 0.007\n",
    "iters_pgd = 10\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Generate and save perturbed data\n",
    "num_batches = len(train_gen)\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    x_batch, y_batch = next(train_gen)\n",
    "    \n",
    "    # Get indices and paths for this batch\n",
    "    batch_indices = train_gen.index_array[i*32 : (i+1)*32]\n",
    "    paths = [train_gen.filepaths[idx] for idx in batch_indices]\n",
    "\n",
    "    # Split batch: 50% FGSM, 50% PGD\n",
    "    split = len(x_batch) // 2\n",
    "\n",
    "    # Generate FGSM samples\n",
    "    x_fgsm = fgsm_attack(\n",
    "        baseline_model, \n",
    "        tf.convert_to_tensor(x_batch[:split]), \n",
    "        tf.convert_to_tensor(y_batch[:split]), \n",
    "        epsilon_fgsm\n",
    "    ).numpy()\n",
    "\n",
    "    # Generate PGD samples\n",
    "    x_pgd = pgd_attack(\n",
    "        tf.convert_to_tensor(x_batch[split:]), \n",
    "        y_batch[split:], \n",
    "        baseline_model, \n",
    "        loss_fn,\n",
    "        epsilon=epsilon_pgd, \n",
    "        alpha=alpha_pgd, \n",
    "        iters=iters_pgd\n",
    "    ).numpy()\n",
    "\n",
    "    # Combine and save\n",
    "    x_perturbed = np.concatenate([x_fgsm, x_pgd], axis=0)\n",
    "\n",
    "    # Save images\n",
    "    for j, path in enumerate(paths):\n",
    "        class_name = 'REAL' if 'REAL' in path else 'FAKE'\n",
    "        filename = os.path.basename(path).replace('.jpg', f'_perturbed_{i}_{j}.jpg')\n",
    "        save_path = os.path.join(perturbed_dir, class_name, filename)\n",
    "        tf.keras.preprocessing.image.save_img(save_path, x_perturbed[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Perturbed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T18:39:29.166736Z",
     "iopub.status.busy": "2025-04-15T18:39:29.166012Z",
     "iopub.status.idle": "2025-04-15T19:25:19.361492Z",
     "shell.execute_reply": "2025-04-15T19:25:19.360660Z",
     "shell.execute_reply.started": "2025-04-15T18:39:29.166709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744742373.180338     339 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1744742373.181059     339 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744742379.193533     401 service.cc:148] XLA service 0x7f47b400e220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744742379.199739     401 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1744742379.199771     401 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1744742379.749710     401 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/3125\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13:44:42\u001b[0m 16s/step - accuracy: 0.5625 - loss: 5.8884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744742391.808150     401 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m910s\u001b[0m 286ms/step - accuracy: 0.7026 - loss: 1.5068 - val_accuracy: 0.6962 - val_loss: 0.7808\n",
      "Epoch 2/3\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m915s\u001b[0m 293ms/step - accuracy: 0.7626 - loss: 0.4895 - val_accuracy: 0.7009 - val_loss: 0.7069\n",
      "Epoch 3/3\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m918s\u001b[0m 294ms/step - accuracy: 0.7726 - loss: 0.4654 - val_accuracy: 0.6479 - val_loss: 1.4056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f4882f79610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    \"/kaggle/working/train_perturbed_only\",  # Directory with 100% perturbed data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize model (same architecture as baseline)\n",
    "adv_model = build_vgg19()\n",
    "adv_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train exclusively on adversarial data\n",
    "adv_model.fit(perturbed_gen, epochs=3, validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:29:20.705535Z",
     "iopub.status.busy": "2025-04-15T17:29:20.705355Z",
     "iopub.status.idle": "2025-04-15T17:29:21.253898Z",
     "shell.execute_reply": "2025-04-15T17:29:21.253289Z",
     "shell.execute_reply.started": "2025-04-15T17:29:20.705515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "    weights_path_adv = '/kaggle/working/vgg19_adv_cifake.weights.h5'\n",
    "    model_path_adv   = '/kaggle/working/vgg19_adv_cifake_model.h5'\n",
    "    \n",
    "    adv_model.save_weights(weights_path_adv)\n",
    "    adv_model.save(model_path_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T19:59:32.692662Z",
     "iopub.status.busy": "2025-04-15T19:59:32.692297Z",
     "iopub.status.idle": "2025-04-15T20:02:06.099731Z",
     "shell.execute_reply": "2025-04-15T20:02:06.099034Z",
     "shell.execute_reply.started": "2025-04-15T19:59:32.692636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Accuracy: 0.6515\n",
      "FGSM Test Accuracy: 0.7500 (Baseline was ~12.5%)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on clean test data (optional)\n",
    "clean_loss, clean_acc = adv_model.evaluate(test_gen, verbose=0)\n",
    "print(f\"Clean Test Accuracy: {clean_acc:.4f}\")\n",
    "\n",
    "# Evaluate on FGSM-attacked test data\n",
    "x_test, y_test = next(test_gen)\n",
    "x_fgsm_test = fgsm_attack(adv_model, tf.convert_to_tensor(x_test), tf.convert_to_tensor(y_test), 0.05)\n",
    "fgsm_loss, fgsm_acc = adv_model.evaluate(x_fgsm_test, y_test, verbose=0)\n",
    "print(f\"FGSM Test Accuracy: {fgsm_acc:.4f} (Baseline was ~12.5%)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3041726,
     "sourceId": 5256696,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 304720,
     "modelInstanceId": 283878,
     "sourceId": 339470,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
