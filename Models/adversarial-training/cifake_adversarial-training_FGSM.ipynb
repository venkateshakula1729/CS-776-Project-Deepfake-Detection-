{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726},{"sourceId":335042,"sourceType":"modelInstanceVersion","modelInstanceId":280459,"modelId":301363},{"sourceId":339508,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":283914,"modelId":304756}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ✅ Imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:21:16.515771Z","iopub.execute_input":"2025-04-15T14:21:16.516039Z","iopub.status.idle":"2025-04-15T14:21:31.677071Z","shell.execute_reply.started":"2025-04-15T14:21:16.516012Z","shell.execute_reply":"2025-04-15T14:21:31.676432Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 14:21:18.349580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744726878.612004      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744726878.689520      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"os.listdir('/kaggle/input') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:21:37.006644Z","iopub.execute_input":"2025-04-15T14:21:37.007367Z","iopub.status.idle":"2025-04-15T14:21:37.013929Z","shell.execute_reply.started":"2025-04-15T14:21:37.007339Z","shell.execute_reply":"2025-04-15T14:21:37.013272Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['cifake-real-and-ai-generated-synthetic-images',\n 'vgg19_adv_cifake_epsilon0.05']"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# ✅ Model Builder","metadata":{}},{"cell_type":"code","source":"def build_vgg19():\n    base = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    for layer in base.layers:\n        layer.trainable = False\n    x = Flatten()(base.output)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    out = Dense(1, activation='sigmoid')(x)\n    return Model(base.input, out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:25:06.618801Z","iopub.execute_input":"2025-04-15T14:25:06.619431Z","iopub.status.idle":"2025-04-15T14:25:06.626495Z","shell.execute_reply.started":"2025-04-15T14:25:06.619405Z","shell.execute_reply":"2025-04-15T14:25:06.625612Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# ✅ Data Generators","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom sklearn.model_selection import train_test_split\n\nbase_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\nval_dir = \"/kaggle/working/validation\"\n\nfor label in ['REAL', 'FAKE']:\n    os.makedirs(os.path.join(val_dir, label), exist_ok=True)\n    files = os.listdir(os.path.join(base_dir, label))\n    train_files, val_files = train_test_split(files, test_size=0.2, random_state=42)\n\n    for fname in val_files:\n        src = os.path.join(base_dir, label, fname)\n        dst = os.path.join(val_dir, label, fname)\n        shutil.copyfile(src, dst)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:25:19.488331Z","iopub.execute_input":"2025-04-15T14:25:19.488705Z","iopub.status.idle":"2025-04-15T14:25:46.506276Z","shell.execute_reply.started":"2025-04-15T14:25:19.488679Z","shell.execute_reply":"2025-04-15T14:25:46.505633Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n    '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train',\n    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=True)\n\nval_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n    '/kaggle/working/validation',\n    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=False)\n\ntest_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n    '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test',\n    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:28:45.457240Z","iopub.execute_input":"2025-04-15T14:28:45.457558Z","iopub.status.idle":"2025-04-15T14:29:30.485385Z","shell.execute_reply.started":"2025-04-15T14:28:45.457536Z","shell.execute_reply":"2025-04-15T14:29:30.484554Z"}},"outputs":[{"name":"stdout","text":"Found 100000 images belonging to 2 classes.\nFound 20000 images belonging to 2 classes.\nFound 20000 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# ✅ FGSM Attack Function and PGD Attack Function","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef fgsm_attack(model, images, labels, epsilon=0.01):\n    with tf.GradientTape() as tape:\n        tape.watch(images)\n        predictions = model(images)\n        loss = tf.keras.losses.BinaryCrossentropy()(labels, predictions)\n    gradient = tape.gradient(loss, images)\n    signed_grad = tf.sign(gradient)\n    adv_images = images + epsilon * signed_grad\n    return tf.clip_by_value(adv_images, 0, 1)\n\n# ✅ PGD Attack Function\n@tf.function\ndef pgd_attack(x, y, model, loss_fn, epsilon=0.01, alpha=0.007, iters=10):\n    x_adv = tf.identity(x)\n\n    for i in range(iters):\n        with tf.GradientTape() as tape:\n            tape.watch(x_adv)\n            prediction = model(x_adv, training=False)\n            loss = loss_fn(y, prediction)\n\n        grad = tape.gradient(loss, x_adv)\n        signed_grad = tf.sign(grad)\n        x_adv = x_adv + alpha * signed_grad\n        x_adv = tf.clip_by_value(x_adv, x - epsilon, x + epsilon)\n        x_adv = tf.clip_by_value(x_adv, 0.0, 1.0)  # ensure valid pixel range\n\n    return x_adv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:43:32.312301Z","iopub.execute_input":"2025-04-15T14:43:32.312838Z","iopub.status.idle":"2025-04-15T14:43:32.328917Z","shell.execute_reply.started":"2025-04-15T14:43:32.312812Z","shell.execute_reply":"2025-04-15T14:43:32.328279Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# ✅ Train and Save Baseline Model","metadata":{}},{"cell_type":"code","source":"# Paths to save model and weights\nweights_path_baseline = '/kaggle/working/vgg19_baseline_cifake.weights.h5'\nmodel_path_baseline   = '/kaggle/working/vgg19_baseline_cifake_model.h5'\n#Kaggle\nif os.path.exists('/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5'):\n    print(\"Loading saved model...\")\n    baseline_model = load_model('/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5')\n#Colab\n# if os.path.exists(model_path_baseline):\n#     print(\"Loading saved model...\")\n#     baseline_model = load_model(model_path_baseline)\nelse:\n    print(\"Training baseline model...\")\n    baseline_model = build_vgg19()\n    baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    baseline_model.fit(train_gen, epochs=3, validation_data=val_gen)\n    baseline_model.save_weights(weights_path_baseline)\n    baseline_model.save(model_path_baseline)\n    print(\"Baseline model trained and saved.\")\n\nprint(\"\\nEvaluating Baseline Model on Clean Test Set:\")\nbaseline_clean_acc = baseline_model.evaluate(test_gen)\nprint(\"Baseline Clean Accuracy:\", baseline_clean_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:04:48.465127Z","iopub.execute_input":"2025-04-15T15:04:48.465479Z","iopub.status.idle":"2025-04-15T15:07:00.162053Z","shell.execute_reply.started":"2025-04-15T15:04:48.465455Z","shell.execute_reply":"2025-04-15T15:07:00.161268Z"}},"outputs":[{"name":"stdout","text":"Loading saved model...\n\nEvaluating Baseline Model on Clean Test Set:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 206ms/step - accuracy: 0.8809 - loss: 0.2490\nBaseline Clean Accuracy: [0.20891635119915009, 0.913349986076355]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Paths to save model and weights\nweights_path_baseline = '/kaggle/working/vgg19_baseline_cifake.weights.h5'\nmodel_path_baseline   = '/kaggle/working/vgg19_baseline_cifake_model.h5'\n\n#Kaggle\nif os.path.exists('/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5'):\n    print(\"Loading saved model...\")\n    baseline_model = load_model('/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5')\nelse:\n    print(\"Training baseline model...\")\n    baseline_model = build_vgg19()\n    baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    baseline_model.fit(train_gen, epochs=3, validation_data=val_gen)\n    baseline_model.save_weights(weights_path_baseline)\n    baseline_model.save(model_path_baseline)\n    print(\"Baseline model trained and saved.\")\n\n# Get a batch of test data\ntest_images, test_labels = next(test_gen)\ntest_images = tf.convert_to_tensor(test_images)\ntest_labels = tf.convert_to_tensor(test_labels)\n\n\n\n# Define epsilon values to test\nepsilons = [0.01, 0.03, 0.05, 0.1]\n\n# Evaluation function using YOUR original attack functions\ndef evaluate_attacks(model, images, labels, epsilons):\n    for eps in epsilons:\n        print(f\"\\nEvaluating for ε = {eps:.2f}\")\n        \n        # FGSM Evaluation\n        adv_images_fgsm = fgsm_attack(model, images, labels, epsilon=eps)\n        _, acc_fgsm = model.evaluate(adv_images_fgsm, labels, verbose=0)\n        print(f\"FGSM Accuracy (ε={eps:.2f}): {acc_fgsm:.4f}\")\n        \n        # PGD Evaluation (using YOUR original PGD function)\n        adv_images_pgd = pgd_attack(images, labels, model, \n                                  loss_fn=tf.keras.losses.BinaryCrossentropy(),\n                                  epsilon=eps, alpha=0.007, iters=10)\n        _, acc_pgd = model.evaluate(adv_images_pgd, labels, verbose=0)\n        print(f\"PGD Accuracy (ε={eps:.2f}): {acc_pgd:.4f}\")\n\n# Run evaluations\nprint(\"\\nEvaluating Adversarial Robustness:\")\nevaluate_attacks(baseline_model, test_images, test_labels, epsilons)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:58:46.285190Z","iopub.execute_input":"2025-04-15T14:58:46.285532Z","iopub.status.idle":"2025-04-15T14:59:53.418284Z","shell.execute_reply.started":"2025-04-15T14:58:46.285509Z","shell.execute_reply":"2025-04-15T14:59:53.417284Z"}},"outputs":[{"name":"stdout","text":"Loading saved model...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1744729128.509399      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1744729128.510207      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating Baseline Model on Clean Test Set:\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1744729132.466827     122 service.cc:148] XLA service 0x78cde8106180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1744729132.467833     122 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1744729132.467854     122 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1744729132.715158     122 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - accuracy: 0.9375 - loss: 0.3287\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1744729143.111693     122 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Baseline Clean Accuracy: 0.9375\n\nEvaluating Adversarial Robustness:\n\nEvaluating for ε = 0.01\nFGSM Accuracy (ε=0.01): 0.1250\nPGD Accuracy (ε=0.01): 0.1250\n\nEvaluating for ε = 0.03\nFGSM Accuracy (ε=0.03): 0.1250\nPGD Accuracy (ε=0.03): 0.0938\n\nEvaluating for ε = 0.05\nFGSM Accuracy (ε=0.05): 0.1250\nPGD Accuracy (ε=0.05): 0.0938\n\nEvaluating for ε = 0.10\nFGSM Accuracy (ε=0.10): 0.1250\nPGD Accuracy (ε=0.10): 0.0938\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Get test data (EXACTLY as you specified)\nx_test_sample, y_test_sample = next(test_gen)\nx_test_tensor = tf.convert_to_tensor(x_test_sample, dtype=tf.float32)\ny_test_tensor = tf.convert_to_tensor(y_test_sample, dtype=tf.float32)\n\n# Evaluate clean accuracy first\nprint(\"\\nEvaluating Baseline Model on Clean Test Set:\")\nclean_loss, clean_acc = baseline_model.evaluate(x_test_tensor, y_test_tensor)\nprint(f\"Clean Accuracy: {clean_acc:.4f}\")\n\n# Define epsilon values\nepsilons = [0.01, 0.03, 0.05, 0.1]\n\n# Evaluate for each epsilon (BUILDING ON YOUR EXACT STRUCTURE)\nfor eps in epsilons:\n    print(f\"\\n=== Evaluating for ε = {eps:.2f} ===\")\n    \n    # FGSM (YOUR EXACT FORMAT)\n    x_fgsm = fgsm_attack(baseline_model, x_test_tensor, y_test_tensor, epsilon=eps)\n    print(\"\\nEvaluating Model on FGSM-Adversarial Examples:\")\n    fgsm_loss, fgsm_acc = baseline_model.evaluate(x_fgsm, y_test_tensor)\n    print(f\"FGSM Accuracy (ε={eps:.2f}): {fgsm_acc:.4f}\")\n    \n    # PGD (using your original parameters)\n    x_pgd = pgd_attack(x_test_tensor, y_test_tensor, baseline_model,\n                      loss_fn=tf.keras.losses.BinaryCrossentropy(),\n                      epsilon=eps, alpha=0.007, iters=10)\n    print(\"\\nEvaluating Model on PGD-Adversarial Examples:\")\n    pgd_loss, pgd_acc = baseline_model.evaluate(x_pgd, y_test_tensor)\n    print(f\"PGD Accuracy (ε={eps:.2f}): {pgd_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:13:09.645011Z","iopub.execute_input":"2025-04-15T15:13:09.645570Z","iopub.status.idle":"2025-04-15T15:13:52.729950Z","shell.execute_reply.started":"2025-04-15T15:13:09.645544Z","shell.execute_reply":"2025-04-15T15:13:52.729207Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating Baseline Model on Clean Test Set:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.8750 - loss: 0.2359\nClean Accuracy: 0.8750\n\n=== Evaluating for ε = 0.01 ===\n\nEvaluating Model on FGSM-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.1562 - loss: 1.0998\nFGSM Accuracy (ε=0.01): 0.1562\n\nEvaluating Model on PGD-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.1562 - loss: 1.1125\nPGD Accuracy (ε=0.01): 0.1562\n\n=== Evaluating for ε = 0.03 ===\n\nEvaluating Model on FGSM-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.1562 - loss: 1.1002\nFGSM Accuracy (ε=0.03): 0.1562\n\nEvaluating Model on PGD-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.1250 - loss: 1.1389\nPGD Accuracy (ε=0.03): 0.1250\n\n=== Evaluating for ε = 0.05 ===\n\nEvaluating Model on FGSM-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.1562 - loss: 1.1003\nFGSM Accuracy (ε=0.05): 0.1562\n\nEvaluating Model on PGD-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.1250 - loss: 1.1640\nPGD Accuracy (ε=0.05): 0.1250\n\n=== Evaluating for ε = 0.10 ===\n\nEvaluating Model on FGSM-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.1562 - loss: 1.1021\nFGSM Accuracy (ε=0.10): 0.1562\n\nEvaluating Model on PGD-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.1250 - loss: 1.1862\nPGD Accuracy (ε=0.10): 0.1250\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# ✅ FGSM Vulnerability Evaluation","metadata":{}},{"cell_type":"code","source":"x_test_sample, y_test_sample = next(test_gen)\nx_test_tensor = tf.convert_to_tensor(x_test_sample, dtype=tf.float32)\ny_test_tensor = tf.convert_to_tensor(y_test_sample, dtype=tf.float32)\n\nx_fgsm_test = fgsm_attack(baseline_model, x_test_tensor, y_test_tensor)\nprint(\"\\nEvaluating Baseline Model on FGSM-Adversarial Examples:\")\nbaseline_fgsm_acc = baseline_model.evaluate(x_fgsm_test, y_test_sample)\nprint(\"Baseline Accuracy on FGSM-Adversarial:\", baseline_fgsm_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:14:24.927991Z","iopub.execute_input":"2025-04-15T15:14:24.928330Z","iopub.status.idle":"2025-04-15T15:14:25.737030Z","shell.execute_reply.started":"2025-04-15T15:14:24.928306Z","shell.execute_reply":"2025-04-15T15:14:25.736250Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating Baseline Model on FGSM-Adversarial Examples:\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.1250 - loss: 1.1114\nBaseline Accuracy on FGSM-Adversarial: [1.1114184856414795, 0.125]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Generate and Save Perturbed Data (FGSM + PGD)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\n# Directories\nbase_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\nperturbed_dir = \"/kaggle/working/train_perturbed_only\"  # All perturbed, no clean data\nweights_path_baseline = \"/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5\"\n\n# Create directories\nos.makedirs(os.path.join(perturbed_dir, 'REAL'), exist_ok=True)\nos.makedirs(os.path.join(perturbed_dir, 'FAKE'), exist_ok=True)\n\n# Load baseline model (for generating perturbations)\nbaseline_model = build_vgg19()\nbaseline_model.load_weights(weights_path_baseline)  # Pretrained weights\n\n# Data generator (no shuffling for consistent file mapping)\ntrain_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n    base_train_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary',\n    shuffle=False  # Critical for matching files to paths\n)\n\n# Attack parameters\nepsilon_fgsm = 0.05\nepsilon_pgd = 0.03\nalpha_pgd = 0.007\niters_pgd = 10\nloss_fn = tf.keras.losses.BinaryCrossentropy()\n\n# Generate and save perturbed data\nnum_batches = len(train_gen)\n\nfor i in tqdm(range(num_batches)):\n    x_batch, y_batch = next(train_gen)\n    \n    # Get indices and paths for this batch\n    batch_indices = train_gen.index_array[i*32 : (i+1)*32]\n    paths = [train_gen.filepaths[idx] for idx in batch_indices]\n\n    # Split batch: 50% FGSM, 50% PGD\n    split = len(x_batch) // 1\n\n    # Generate FGSM samples\n    x_fgsm = fgsm_attack(\n        baseline_model, \n        tf.convert_to_tensor(x_batch[:split]), \n        tf.convert_to_tensor(y_batch[:split]), \n        epsilon_fgsm\n    ).numpy()\n\n    # Generate PGD samples\n    # x_pgd = pgd_attack(\n    #     tf.convert_to_tensor(x_batch[split:]), \n    #     y_batch[split:], \n    #     baseline_model, \n    #     loss_fn,\n    #     epsilon=epsilon_pgd, \n    #     alpha=alpha_pgd, \n    #     iters=iters_pgd\n    # ).numpy()\n\n    # Combine and save\n    # x_perturbed = np.concatenate([x_fgsm, x_pgd], axis=0)\n    x_perturbed = x_fgsm\n    # Save images\n    for j, path in enumerate(paths):\n        class_name = 'REAL' if 'REAL' in path else 'FAKE'\n        filename = os.path.basename(path).replace('.jpg', f'_perturbed_{i}_{j}.jpg')\n        save_path = os.path.join(perturbed_dir, class_name, filename)\n        tf.keras.preprocessing.image.save_img(save_path, x_perturbed[j])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:17:39.917501Z","iopub.execute_input":"2025-04-15T15:17:39.918277Z","iopub.status.idle":"2025-04-15T15:19:12.994474Z","shell.execute_reply.started":"2025-04-15T15:17:39.918254Z","shell.execute_reply":"2025-04-15T15:19:12.993576Z"}},"outputs":[{"name":"stdout","text":"Found 100000 images belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 64/3125 [00:36<28:54,  1.76it/s] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/513954687.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mepsilon_fgsm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     ).numpy()\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Generate PGD samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayLike\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport random\n\n# --- Original directory setup ---\nbase_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\nperturbed_dir = \"/kaggle/working/train_perturbed_only\"\nweights_path_baseline = \"/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5\"\n\n# Correct directory creation\nos.makedirs(os.path.join(perturbed_dir, 'REAL'), exist_ok=True)\nos.makedirs(os.path.join(perturbed_dir, 'FAKE'), exist_ok=True)\n# --- Load model --- \nbaseline_model = build_vgg19()\nbaseline_model.load_weights(weights_path_baseline)\n\n# --- Get all file paths ---\nall_filepaths = []\nfor root, dirs, files in os.walk(base_train_dir):\n    for file in files:\n        if file.endswith('.jpg'):\n            all_filepaths.append(os.path.join(root, file))\n\n# Select 1/4 of files (preserving class balance)\nreal_files = [f for f in all_filepaths if 'REAL' in f]\nfake_files = [f for f in all_filepaths if 'FAKE' in f]\nselected_files = (\n    random.sample(real_files, len(real_files)//4) + \n    random.sample(fake_files, len(fake_files)//4)\n)\n\nprint(f\"Original dataset: {len(all_filepaths)} images\")\nprint(f\"Using subset: {len(selected_files)} images (1/4 of original)\\n\")\n\n# --- Create custom generator for subset ---\nclass SubsetGenerator(ImageDataGenerator):\n    def flow_from_directory(self, directory, subset_files, **kwargs):\n        generator = super().flow_from_directory(directory, **kwargs)\n        # Filter to only include our selected files\n        mask = [f in subset_files for f in generator.filepaths]\n        generator.filepaths = [f for f, m in zip(generator.filepaths, mask) if m]\n        generator.classes = [c for c, m in zip(generator.classes, mask) if m]\n        generator.samples = len(generator.filepaths)\n        return generator\n\n# Create generator with subset\ntrain_gen = SubsetGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n    base_train_dir,\n    subset_files=selected_files,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary',\n    shuffle=False\n)\n\n# --- Processing loop with progress bar ---\nnum_batches = len(train_gen)\nprint(f\"\\nProcessing {num_batches} batches...\")\nwith tqdm(total=num_batches, unit='batch') as pbar:\n    for i in range(num_batches):\n        x_batch, y_batch = next(train_gen)\n        \n        # Generate FGSM samples\n        x_fgsm = fgsm_attack(\n            baseline_model, \n            tf.convert_to_tensor(x_batch), \n            tf.convert_to_tensor(y_batch), \n            epsilon=0.05\n        ).numpy()\n\n        # Save images\n        batch_indices = train_gen.index_array[i*32 : (i+1)*32]\n        paths = [train_gen.filepaths[idx] for idx in batch_indices]\n        for j, path in enumerate(paths):\n            class_name = 'REAL' if 'REAL' in path else 'FAKE'\n            filename = os.path.basename(path).replace('.jpg', f'_perturbed_{i}_{j}.jpg')\n            save_path = os.path.join(perturbed_dir, class_name, filename)\n            tf.keras.preprocessing.image.save_img(save_path, x_fgsm[j])\n        \n        pbar.update(1)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:17:06.129127Z","iopub.execute_input":"2025-04-15T16:17:06.129368Z","iopub.status.idle":"2025-04-15T16:17:25.050163Z","shell.execute_reply.started":"2025-04-15T16:17:06.129350Z","shell.execute_reply":"2025-04-15T16:17:25.049152Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3741193921.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filepaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_train_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0mreal_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'REAL'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mfake_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'FAKE'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3741193921.py\u001b[0m in \u001b[0;36mget_filepaths\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_filepaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mfilepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n\n# --- Custom data loading ---\ndef load_and_preprocess_image(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [224, 224])\n    img = preprocess_input(img)\n    return img\n\n# Process images in batches\nbatch_size = 32\nnum_batches = len(selected_files) // batch_size\n\nprint(f\"Processing {num_batches} batches...\")\nwith tqdm(total=num_batches, unit='batch') as pbar:\n    for i in range(num_batches):\n        batch_files = selected_files[i*batch_size : (i+1)*batch_size]\n        \n        # Load and preprocess batch\n        x_batch = np.array([load_and_preprocess_image(f).numpy() for f in batch_files])\n        y_batch = np.array([0 if 'REAL' in f else 1 for f in batch_files])\n        \n        # Generate FGSM samples\n        x_fgsm = fgsm_attack(\n            baseline_model,\n            tf.convert_to_tensor(x_batch),\n            tf.convert_to_tensor(y_batch),\n            epsilon=0.05\n        ).numpy()\n        \n        # Save images\n        for j, path in enumerate(batch_files):\n            class_name = 'REAL' if 'REAL' in path else 'FAKE'\n            filename = os.path.basename(path).replace('.jpg', f'_perturbed_{i}_{j}.jpg')\n            save_path = os.path.join(perturbed_dir, class_name, filename)\n            array_to_img(x_fgsm[j]).save(save_path)\n        \n        pbar.update(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:36:27.060170Z","iopub.execute_input":"2025-04-15T15:36:27.060990Z","iopub.status.idle":"2025-04-15T15:45:59.170285Z","shell.execute_reply.started":"2025-04-15T15:36:27.060955Z","shell.execute_reply":"2025-04-15T15:45:59.169375Z"}},"outputs":[{"name":"stdout","text":"Processing 781 batches...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 781/781 [09:32<00:00,  1.37batch/s]\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Train on Perturbed Data","metadata":{}},{"cell_type":"code","source":"perturbed_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n    perturbed_dir,  # Directory with 100% perturbed data\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary',\n    shuffle=True\n)\n\n# Initialize model (same architecture as baseline)\nadv_model = build_vgg19()\nadv_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train exclusively on adversarial data\nadv_model.fit(perturbed_gen, epochs=3, validation_data=val_gen)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:01:32.576713Z","iopub.execute_input":"2025-04-15T16:01:32.577009Z","iopub.status.idle":"2025-04-15T16:17:03.823495Z","shell.execute_reply.started":"2025-04-15T16:01:32.576981Z","shell.execute_reply":"2025-04-15T16:17:03.822818Z"}},"outputs":[{"name":"stdout","text":"Found 27040 images belonging to 2 classes.\nEpoch 1/3\n\u001b[1m845/845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 366ms/step - accuracy: 0.7232 - loss: 2.5260 - val_accuracy: 0.5095 - val_loss: 3.6678\nEpoch 2/3\n\u001b[1m845/845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 365ms/step - accuracy: 0.8128 - loss: 0.4012 - val_accuracy: 0.5042 - val_loss: 5.8864\nEpoch 3/3\n\u001b[1m845/845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 365ms/step - accuracy: 0.8362 - loss: 0.3519 - val_accuracy: 0.5140 - val_loss: 5.3942\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x78cb3cfbfe90>"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Saving the Model","metadata":{}},{"cell_type":"code","source":"weights_path_adv = '/kaggle/working/vgg19_adv_FGSMonly_cifake.weights.h5'\nmodel_path_adv   = '/kaggle/working/vgg19_adv_FGSMonly_cifake_model.h5'\n\nadv_model.save_weights(weights_path_adv)\nadv_model.save(model_path_adv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:17:05.291905Z","iopub.execute_input":"2025-04-15T16:17:05.292177Z","iopub.status.idle":"2025-04-15T16:17:06.128136Z","shell.execute_reply.started":"2025-04-15T16:17:05.292157Z","shell.execute_reply":"2025-04-15T16:17:06.127251Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Evaluate Robustness","metadata":{}},{"cell_type":"code","source":"# Evaluate on clean test data (optional)\nclean_loss, clean_acc = adv_model.evaluate(test_gen, verbose=0)\nprint(f\"Clean Test Accuracy: {clean_acc:.4f}\")\n\n# Evaluate on FGSM-attacked test data\nx_test, y_test = next(test_gen)\nx_fgsm_test = fgsm_attack(adv_model, tf.convert_to_tensor(x_test), tf.convert_to_tensor(y_test), epsilon_fgsm)\nfgsm_loss, fgsm_acc = adv_model.evaluate(x_fgsm_test, y_test, verbose=0)\nprint(f\"FGSM Test Accuracy: {fgsm_acc:.4f} (Baseline was ~12.5%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:17:35.889356Z","iopub.execute_input":"2025-04-15T16:17:35.890087Z","iopub.status.idle":"2025-04-15T16:19:47.834295Z","shell.execute_reply.started":"2025-04-15T16:17:35.890062Z","shell.execute_reply":"2025-04-15T16:19:47.833517Z"}},"outputs":[{"name":"stdout","text":"Clean Test Accuracy: 0.5147\nFGSM Test Accuracy: 0.2188 (Baseline was ~12.5%)\n","output_type":"stream"}],"execution_count":29}]}