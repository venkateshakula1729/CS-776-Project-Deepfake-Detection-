{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:21:16.516039Z",
     "iopub.status.busy": "2025-04-15T14:21:16.515771Z",
     "iopub.status.idle": "2025-04-15T14:21:31.677071Z",
     "shell.execute_reply": "2025-04-15T14:21:31.676432Z",
     "shell.execute_reply.started": "2025-04-15T14:21:16.516012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:21:18.349580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744726878.612004      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744726878.689520      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:21:37.007367Z",
     "iopub.status.busy": "2025-04-15T14:21:37.006644Z",
     "iopub.status.idle": "2025-04-15T14:21:37.013929Z",
     "shell.execute_reply": "2025-04-15T14:21:37.013272Z",
     "shell.execute_reply.started": "2025-04-15T14:21:37.007339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cifake-real-and-ai-generated-synthetic-images',\n",
       " 'vgg19_adv_cifake_epsilon0.05']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/kaggle/input') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:25:06.619431Z",
     "iopub.status.busy": "2025-04-15T14:25:06.618801Z",
     "iopub.status.idle": "2025-04-15T14:25:06.626495Z",
     "shell.execute_reply": "2025-04-15T14:25:06.625612Z",
     "shell.execute_reply.started": "2025-04-15T14:25:06.619405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_vgg19():\n",
    "    base = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "    x = Flatten()(base.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(base.input, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:25:19.488705Z",
     "iopub.status.busy": "2025-04-15T14:25:19.488331Z",
     "iopub.status.idle": "2025-04-15T14:25:46.506276Z",
     "shell.execute_reply": "2025-04-15T14:25:46.505633Z",
     "shell.execute_reply.started": "2025-04-15T14:25:19.488679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\n",
    "val_dir = \"/kaggle/working/validation\"\n",
    "\n",
    "for label in ['REAL', 'FAKE']:\n",
    "    os.makedirs(os.path.join(val_dir, label), exist_ok=True)\n",
    "    files = os.listdir(os.path.join(base_dir, label))\n",
    "    train_files, val_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "\n",
    "    for fname in val_files:\n",
    "        src = os.path.join(base_dir, label, fname)\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:28:45.457558Z",
     "iopub.status.busy": "2025-04-15T14:28:45.457240Z",
     "iopub.status.idle": "2025-04-15T14:29:30.485385Z",
     "shell.execute_reply": "2025-04-15T14:29:30.484554Z",
     "shell.execute_reply.started": "2025-04-15T14:28:45.457536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train',\n",
    "    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=True)\n",
    "\n",
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    '/kaggle/working/validation',\n",
    "    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=False)\n",
    "\n",
    "test_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    '/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test',\n",
    "    target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ FGSM Attack Function and PGD Attack Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:43:32.312838Z",
     "iopub.status.busy": "2025-04-15T14:43:32.312301Z",
     "iopub.status.idle": "2025-04-15T14:43:32.328917Z",
     "shell.execute_reply": "2025-04-15T14:43:32.328279Z",
     "shell.execute_reply.started": "2025-04-15T14:43:32.312812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def fgsm_attack(model, images, labels, epsilon=0.01):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        predictions = model(images)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()(labels, predictions)\n",
    "    gradient = tape.gradient(loss, images)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    adv_images = images + epsilon * signed_grad\n",
    "    return tf.clip_by_value(adv_images, 0, 1)\n",
    "\n",
    "# ✅ PGD Attack Function\n",
    "@tf.function\n",
    "def pgd_attack(x, y, model, loss_fn, epsilon=0.01, alpha=0.007, iters=10):\n",
    "    x_adv = tf.identity(x)\n",
    "\n",
    "    for i in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            prediction = model(x_adv, training=False)\n",
    "            loss = loss_fn(y, prediction)\n",
    "\n",
    "        grad = tape.gradient(loss, x_adv)\n",
    "        signed_grad = tf.sign(grad)\n",
    "        x_adv = x_adv + alpha * signed_grad\n",
    "        x_adv = tf.clip_by_value(x_adv, x - epsilon, x + epsilon)\n",
    "        x_adv = tf.clip_by_value(x_adv, 0.0, 1.0)  # ensure valid pixel range\n",
    "\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Train and Save Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:04:48.465479Z",
     "iopub.status.busy": "2025-04-15T15:04:48.465127Z",
     "iopub.status.idle": "2025-04-15T15:07:00.162053Z",
     "shell.execute_reply": "2025-04-15T15:07:00.161268Z",
     "shell.execute_reply.started": "2025-04-15T15:04:48.465455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model...\n",
      "\n",
      "Evaluating Baseline Model on Clean Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 206ms/step - accuracy: 0.8809 - loss: 0.2490\n",
      "Baseline Clean Accuracy: [0.20891635119915009, 0.913349986076355]\n"
     ]
    }
   ],
   "source": [
    "# Paths to save model and weights\n",
    "weights_path_baseline = '/kaggle/working/vgg19_baseline_cifake.weights.h5'\n",
    "model_path_baseline   = '/kaggle/working/vgg19_baseline_cifake_model.h5'\n",
    "#Kaggle\n",
    "if os.path.exists('/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5'):\n",
    "    print(\"Loading saved model...\")\n",
    "    baseline_model = load_model('/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5')\n",
    "#Colab\n",
    "# if os.path.exists(model_path_baseline):\n",
    "#     print(\"Loading saved model...\")\n",
    "#     baseline_model = load_model(model_path_baseline)\n",
    "else:\n",
    "    print(\"Training baseline model...\")\n",
    "    baseline_model = build_vgg19()\n",
    "    baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    baseline_model.fit(train_gen, epochs=3, validation_data=val_gen)\n",
    "    baseline_model.save_weights(weights_path_baseline)\n",
    "    baseline_model.save(model_path_baseline)\n",
    "    print(\"Baseline model trained and saved.\")\n",
    "\n",
    "print(\"\\nEvaluating Baseline Model on Clean Test Set:\")\n",
    "baseline_clean_acc = baseline_model.evaluate(test_gen)\n",
    "print(\"Baseline Clean Accuracy:\", baseline_clean_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:58:46.285532Z",
     "iopub.status.busy": "2025-04-15T14:58:46.285190Z",
     "iopub.status.idle": "2025-04-15T14:59:53.418284Z",
     "shell.execute_reply": "2025-04-15T14:59:53.417284Z",
     "shell.execute_reply.started": "2025-04-15T14:58:46.285509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744729128.509399      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1744729128.510207      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Baseline Model on Clean Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744729132.466827     122 service.cc:148] XLA service 0x78cde8106180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744729132.467833     122 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1744729132.467854     122 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1744729132.715158     122 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - accuracy: 0.9375 - loss: 0.3287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744729143.111693     122 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Clean Accuracy: 0.9375\n",
      "\n",
      "Evaluating Adversarial Robustness:\n",
      "\n",
      "Evaluating for ε = 0.01\n",
      "FGSM Accuracy (ε=0.01): 0.1250\n",
      "PGD Accuracy (ε=0.01): 0.1250\n",
      "\n",
      "Evaluating for ε = 0.03\n",
      "FGSM Accuracy (ε=0.03): 0.1250\n",
      "PGD Accuracy (ε=0.03): 0.0938\n",
      "\n",
      "Evaluating for ε = 0.05\n",
      "FGSM Accuracy (ε=0.05): 0.1250\n",
      "PGD Accuracy (ε=0.05): 0.0938\n",
      "\n",
      "Evaluating for ε = 0.10\n",
      "FGSM Accuracy (ε=0.10): 0.1250\n",
      "PGD Accuracy (ε=0.10): 0.0938\n"
     ]
    }
   ],
   "source": [
    "# Paths to save model and weights\n",
    "weights_path_baseline = '/kaggle/working/vgg19_baseline_cifake.weights.h5'\n",
    "model_path_baseline   = '/kaggle/working/vgg19_baseline_cifake_model.h5'\n",
    "\n",
    "#Kaggle\n",
    "if os.path.exists('/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5'):\n",
    "    print(\"Loading saved model...\")\n",
    "    baseline_model = load_model('/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5')\n",
    "else:\n",
    "    print(\"Training baseline model...\")\n",
    "    baseline_model = build_vgg19()\n",
    "    baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    baseline_model.fit(train_gen, epochs=3, validation_data=val_gen)\n",
    "    baseline_model.save_weights(weights_path_baseline)\n",
    "    baseline_model.save(model_path_baseline)\n",
    "    print(\"Baseline model trained and saved.\")\n",
    "\n",
    "# Get a batch of test data\n",
    "test_images, test_labels = next(test_gen)\n",
    "test_images = tf.convert_to_tensor(test_images)\n",
    "test_labels = tf.convert_to_tensor(test_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Define epsilon values to test\n",
    "epsilons = [0.01, 0.03, 0.05, 0.1]\n",
    "\n",
    "# Evaluation function using YOUR original attack functions\n",
    "def evaluate_attacks(model, images, labels, epsilons):\n",
    "    for eps in epsilons:\n",
    "        print(f\"\\nEvaluating for ε = {eps:.2f}\")\n",
    "        \n",
    "        # FGSM Evaluation\n",
    "        adv_images_fgsm = fgsm_attack(model, images, labels, epsilon=eps)\n",
    "        _, acc_fgsm = model.evaluate(adv_images_fgsm, labels, verbose=0)\n",
    "        print(f\"FGSM Accuracy (ε={eps:.2f}): {acc_fgsm:.4f}\")\n",
    "        \n",
    "        # PGD Evaluation (using YOUR original PGD function)\n",
    "        adv_images_pgd = pgd_attack(images, labels, model, \n",
    "                                  loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                  epsilon=eps, alpha=0.007, iters=10)\n",
    "        _, acc_pgd = model.evaluate(adv_images_pgd, labels, verbose=0)\n",
    "        print(f\"PGD Accuracy (ε={eps:.2f}): {acc_pgd:.4f}\")\n",
    "\n",
    "# Run evaluations\n",
    "print(\"\\nEvaluating Adversarial Robustness:\")\n",
    "evaluate_attacks(baseline_model, test_images, test_labels, epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:13:09.645570Z",
     "iopub.status.busy": "2025-04-15T15:13:09.645011Z",
     "iopub.status.idle": "2025-04-15T15:13:52.729950Z",
     "shell.execute_reply": "2025-04-15T15:13:52.729207Z",
     "shell.execute_reply.started": "2025-04-15T15:13:09.645544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Baseline Model on Clean Test Set:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.8750 - loss: 0.2359\n",
      "Clean Accuracy: 0.8750\n",
      "\n",
      "=== Evaluating for ε = 0.01 ===\n",
      "\n",
      "Evaluating Model on FGSM-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.1562 - loss: 1.0998\n",
      "FGSM Accuracy (ε=0.01): 0.1562\n",
      "\n",
      "Evaluating Model on PGD-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.1562 - loss: 1.1125\n",
      "PGD Accuracy (ε=0.01): 0.1562\n",
      "\n",
      "=== Evaluating for ε = 0.03 ===\n",
      "\n",
      "Evaluating Model on FGSM-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.1562 - loss: 1.1002\n",
      "FGSM Accuracy (ε=0.03): 0.1562\n",
      "\n",
      "Evaluating Model on PGD-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.1250 - loss: 1.1389\n",
      "PGD Accuracy (ε=0.03): 0.1250\n",
      "\n",
      "=== Evaluating for ε = 0.05 ===\n",
      "\n",
      "Evaluating Model on FGSM-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.1562 - loss: 1.1003\n",
      "FGSM Accuracy (ε=0.05): 0.1562\n",
      "\n",
      "Evaluating Model on PGD-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.1250 - loss: 1.1640\n",
      "PGD Accuracy (ε=0.05): 0.1250\n",
      "\n",
      "=== Evaluating for ε = 0.10 ===\n",
      "\n",
      "Evaluating Model on FGSM-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.1562 - loss: 1.1021\n",
      "FGSM Accuracy (ε=0.10): 0.1562\n",
      "\n",
      "Evaluating Model on PGD-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.1250 - loss: 1.1862\n",
      "PGD Accuracy (ε=0.10): 0.1250\n"
     ]
    }
   ],
   "source": [
    "# Get test data (EXACTLY as you specified)\n",
    "x_test_sample, y_test_sample = next(test_gen)\n",
    "x_test_tensor = tf.convert_to_tensor(x_test_sample, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_sample, dtype=tf.float32)\n",
    "\n",
    "# Evaluate clean accuracy first\n",
    "print(\"\\nEvaluating Baseline Model on Clean Test Set:\")\n",
    "clean_loss, clean_acc = baseline_model.evaluate(x_test_tensor, y_test_tensor)\n",
    "print(f\"Clean Accuracy: {clean_acc:.4f}\")\n",
    "\n",
    "# Define epsilon values\n",
    "epsilons = [0.01, 0.03, 0.05, 0.1]\n",
    "\n",
    "# Evaluate for each epsilon (BUILDING ON YOUR EXACT STRUCTURE)\n",
    "for eps in epsilons:\n",
    "    print(f\"\\n=== Evaluating for ε = {eps:.2f} ===\")\n",
    "    \n",
    "    # FGSM (YOUR EXACT FORMAT)\n",
    "    x_fgsm = fgsm_attack(baseline_model, x_test_tensor, y_test_tensor, epsilon=eps)\n",
    "    print(\"\\nEvaluating Model on FGSM-Adversarial Examples:\")\n",
    "    fgsm_loss, fgsm_acc = baseline_model.evaluate(x_fgsm, y_test_tensor)\n",
    "    print(f\"FGSM Accuracy (ε={eps:.2f}): {fgsm_acc:.4f}\")\n",
    "    \n",
    "    # PGD (using your original parameters)\n",
    "    x_pgd = pgd_attack(x_test_tensor, y_test_tensor, baseline_model,\n",
    "                      loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
    "                      epsilon=eps, alpha=0.007, iters=10)\n",
    "    print(\"\\nEvaluating Model on PGD-Adversarial Examples:\")\n",
    "    pgd_loss, pgd_acc = baseline_model.evaluate(x_pgd, y_test_tensor)\n",
    "    print(f\"PGD Accuracy (ε={eps:.2f}): {pgd_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ FGSM Vulnerability Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:14:24.928330Z",
     "iopub.status.busy": "2025-04-15T15:14:24.927991Z",
     "iopub.status.idle": "2025-04-15T15:14:25.737030Z",
     "shell.execute_reply": "2025-04-15T15:14:25.736250Z",
     "shell.execute_reply.started": "2025-04-15T15:14:24.928306Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Baseline Model on FGSM-Adversarial Examples:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.1250 - loss: 1.1114\n",
      "Baseline Accuracy on FGSM-Adversarial: [1.1114184856414795, 0.125]\n"
     ]
    }
   ],
   "source": [
    "x_test_sample, y_test_sample = next(test_gen)\n",
    "x_test_tensor = tf.convert_to_tensor(x_test_sample, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_sample, dtype=tf.float32)\n",
    "\n",
    "x_fgsm_test = fgsm_attack(baseline_model, x_test_tensor, y_test_tensor)\n",
    "print(\"\\nEvaluating Baseline Model on FGSM-Adversarial Examples:\")\n",
    "baseline_fgsm_acc = baseline_model.evaluate(x_fgsm_test, y_test_sample)\n",
    "print(\"Baseline Accuracy on FGSM-Adversarial:\", baseline_fgsm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Save Perturbed Data (FGSM + PGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:17:39.918277Z",
     "iopub.status.busy": "2025-04-15T15:17:39.917501Z",
     "iopub.status.idle": "2025-04-15T15:19:12.994474Z",
     "shell.execute_reply": "2025-04-15T15:19:12.993576Z",
     "shell.execute_reply.started": "2025-04-15T15:17:39.918254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 64/3125 [00:36<28:54,  1.76it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/513954687.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mepsilon_fgsm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     ).numpy()\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Generate PGD samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayLike\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directories\n",
    "base_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\n",
    "perturbed_dir = \"/kaggle/working/train_perturbed_only\"  # All perturbed, no clean data\n",
    "weights_path_baseline = \"/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(os.path.join(perturbed_dir, 'REAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(perturbed_dir, 'FAKE'), exist_ok=True)\n",
    "\n",
    "# Load baseline model (for generating perturbations)\n",
    "baseline_model = build_vgg19()\n",
    "baseline_model.load_weights(weights_path_baseline)  # Pretrained weights\n",
    "\n",
    "# Data generator (no shuffling for consistent file mapping)\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    base_train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False  # Critical for matching files to paths\n",
    ")\n",
    "\n",
    "# Attack parameters\n",
    "epsilon_fgsm = 0.05\n",
    "epsilon_pgd = 0.03\n",
    "alpha_pgd = 0.007\n",
    "iters_pgd = 10\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Generate and save perturbed data\n",
    "num_batches = len(train_gen)\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    x_batch, y_batch = next(train_gen)\n",
    "    \n",
    "    # Get indices and paths for this batch\n",
    "    batch_indices = train_gen.index_array[i*32 : (i+1)*32]\n",
    "    paths = [train_gen.filepaths[idx] for idx in batch_indices]\n",
    "\n",
    "    # Split batch: 50% FGSM, 50% PGD\n",
    "    split = len(x_batch) // 1\n",
    "\n",
    "    # Generate FGSM samples\n",
    "    x_fgsm = fgsm_attack(\n",
    "        baseline_model, \n",
    "        tf.convert_to_tensor(x_batch[:split]), \n",
    "        tf.convert_to_tensor(y_batch[:split]), \n",
    "        epsilon_fgsm\n",
    "    ).numpy()\n",
    "\n",
    "    # Generate PGD samples\n",
    "    # x_pgd = pgd_attack(\n",
    "    #     tf.convert_to_tensor(x_batch[split:]), \n",
    "    #     y_batch[split:], \n",
    "    #     baseline_model, \n",
    "    #     loss_fn,\n",
    "    #     epsilon=epsilon_pgd, \n",
    "    #     alpha=alpha_pgd, \n",
    "    #     iters=iters_pgd\n",
    "    # ).numpy()\n",
    "\n",
    "    # Combine and save\n",
    "    # x_perturbed = np.concatenate([x_fgsm, x_pgd], axis=0)\n",
    "    x_perturbed = x_fgsm\n",
    "    # Save images\n",
    "    for j, path in enumerate(paths):\n",
    "        class_name = 'REAL' if 'REAL' in path else 'FAKE'\n",
    "        filename = os.path.basename(path).replace('.jpg', f'_perturbed_{i}_{j}.jpg')\n",
    "        save_path = os.path.join(perturbed_dir, class_name, filename)\n",
    "        tf.keras.preprocessing.image.save_img(save_path, x_perturbed[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:17:06.129368Z",
     "iopub.status.busy": "2025-04-15T16:17:06.129127Z",
     "iopub.status.idle": "2025-04-15T16:17:25.050163Z",
     "shell.execute_reply": "2025-04-15T16:17:25.049152Z",
     "shell.execute_reply.started": "2025-04-15T16:17:06.129350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3741193921.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filepaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_train_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0mreal_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'REAL'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mfake_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'FAKE'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/3741193921.py\u001b[0m in \u001b[0;36mget_filepaths\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_filepaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mfilepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# --- Original directory setup ---\n",
    "base_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\n",
    "perturbed_dir = \"/kaggle/working/train_perturbed_only\"\n",
    "weights_path_baseline = \"/kaggle/input/vgg19_adv_cifake_epsilon0.05/tensorflow2/default/1/vgg19_baseline_cifake_model.h5\"\n",
    "\n",
    "# Correct directory creation\n",
    "os.makedirs(os.path.join(perturbed_dir, 'REAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(perturbed_dir, 'FAKE'), exist_ok=True)\n",
    "# --- Load model --- \n",
    "baseline_model = build_vgg19()\n",
    "baseline_model.load_weights(weights_path_baseline)\n",
    "\n",
    "# --- Get all file paths ---\n",
    "all_filepaths = []\n",
    "for root, dirs, files in os.walk(base_train_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            all_filepaths.append(os.path.join(root, file))\n",
    "\n",
    "# Select 1/4 of files (preserving class balance)\n",
    "real_files = [f for f in all_filepaths if 'REAL' in f]\n",
    "fake_files = [f for f in all_filepaths if 'FAKE' in f]\n",
    "selected_files = (\n",
    "    random.sample(real_files, len(real_files)//4) + \n",
    "    random.sample(fake_files, len(fake_files)//4)\n",
    ")\n",
    "\n",
    "print(f\"Original dataset: {len(all_filepaths)} images\")\n",
    "print(f\"Using subset: {len(selected_files)} images (1/4 of original)\\n\")\n",
    "\n",
    "# --- Create custom generator for subset ---\n",
    "# class SubsetGenerator(ImageDataGenerator):\n",
    "#     def flow_from_directory(self, directory, subset_files, **kwargs):\n",
    "#         generator = super().flow_from_directory(directory, **kwargs)\n",
    "#         # Filter to only include our selected files\n",
    "#         mask = [f in subset_files for f in generator.filepaths]\n",
    "#         generator.filepaths = [f for f, m in zip(generator.filepaths, mask) if m]\n",
    "#         generator.classes = [c for c, m in zip(generator.classes, mask) if m]\n",
    "#         generator.samples = len(generator.filepaths)\n",
    "#         return generator\n",
    "\n",
    "# Create generator with subset\n",
    "# train_gen = SubsetGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "#     base_train_dir,\n",
    "#     subset_files=selected_files,\n",
    "#     target_size=(224, 224),\n",
    "#     batch_size=32,\n",
    "#     class_mode='binary',\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# # --- Processing loop with progress bar ---\n",
    "# num_batches = len(train_gen)\n",
    "# print(f\"\\nProcessing {num_batches} batches...\")\n",
    "# with tqdm(total=num_batches, unit='batch') as pbar:\n",
    "#     for i in range(num_batches):\n",
    "#         x_batch, y_batch = next(train_gen)\n",
    "        \n",
    "#         # Generate FGSM samples\n",
    "#         x_fgsm = fgsm_attack(\n",
    "#             baseline_model, \n",
    "#             tf.convert_to_tensor(x_batch), \n",
    "#             tf.convert_to_tensor(y_batch), \n",
    "#             epsilon=0.05\n",
    "#         ).numpy()\n",
    "\n",
    "#         # Save images\n",
    "#         batch_indices = train_gen.index_array[i*32 : (i+1)*32]\n",
    "#         paths = [train_gen.filepaths[idx] for idx in batch_indices]\n",
    "#         for j, path in enumerate(paths):\n",
    "#             class_name = 'REAL' if 'REAL' in path else 'FAKE'\n",
    "#             filename = os.path.basename(path).replace('.jpg', f'_perturbed_{i}_{j}.jpg')\n",
    "#             save_path = os.path.join(perturbed_dir, class_name, filename)\n",
    "#             tf.keras.preprocessing.image.save_img(save_path, x_fgsm[j])\n",
    "        \n",
    "#         pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:36:27.060990Z",
     "iopub.status.busy": "2025-04-15T15:36:27.060170Z",
     "iopub.status.idle": "2025-04-15T15:45:59.170285Z",
     "shell.execute_reply": "2025-04-15T15:45:59.169375Z",
     "shell.execute_reply.started": "2025-04-15T15:36:27.060955Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 781 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [09:32<00:00,  1.37batch/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "# --- Custom data loading ---\n",
    "def load_and_preprocess_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224])\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Process images in batches\n",
    "batch_size = 32\n",
    "num_batches = len(selected_files) // batch_size\n",
    "\n",
    "print(f\"Processing {num_batches} batches...\")\n",
    "with tqdm(total=num_batches, unit='batch') as pbar:\n",
    "    for i in range(num_batches):\n",
    "        batch_files = selected_files[i*batch_size : (i+1)*batch_size]\n",
    "        \n",
    "        # Load and preprocess batch\n",
    "        x_batch = np.array([load_and_preprocess_image(f).numpy() for f in batch_files])\n",
    "        y_batch = np.array([0 if 'REAL' in f else 1 for f in batch_files])\n",
    "        \n",
    "        # Generate FGSM samples\n",
    "        x_fgsm = fgsm_attack(\n",
    "            baseline_model,\n",
    "            tf.convert_to_tensor(x_batch),\n",
    "            tf.convert_to_tensor(y_batch),\n",
    "            epsilon=0.05\n",
    "        ).numpy()\n",
    "        \n",
    "        # Save images\n",
    "        for j, path in enumerate(batch_files):\n",
    "            class_name = 'REAL' if 'REAL' in path else 'FAKE'\n",
    "            filename = os.path.basename(path).replace('.jpg', f'_perturbed_{i}_{j}.jpg')\n",
    "            save_path = os.path.join(perturbed_dir, class_name, filename)\n",
    "            array_to_img(x_fgsm[j]).save(save_path)\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Perturbed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:01:32.577009Z",
     "iopub.status.busy": "2025-04-15T16:01:32.576713Z",
     "iopub.status.idle": "2025-04-15T16:17:03.823495Z",
     "shell.execute_reply": "2025-04-15T16:17:03.822818Z",
     "shell.execute_reply.started": "2025-04-15T16:01:32.576981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27040 images belonging to 2 classes.\n",
      "Epoch 1/3\n",
      "\u001b[1m845/845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 366ms/step - accuracy: 0.7232 - loss: 2.5260 - val_accuracy: 0.5095 - val_loss: 3.6678\n",
      "Epoch 2/3\n",
      "\u001b[1m845/845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 365ms/step - accuracy: 0.8128 - loss: 0.4012 - val_accuracy: 0.5042 - val_loss: 5.8864\n",
      "Epoch 3/3\n",
      "\u001b[1m845/845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 365ms/step - accuracy: 0.8362 - loss: 0.3519 - val_accuracy: 0.5140 - val_loss: 5.3942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x78cb3cfbfe90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_gen = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(\n",
    "    perturbed_dir,  # Directory with 100% perturbed data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize model (same architecture as baseline)\n",
    "adv_model = build_vgg19()\n",
    "adv_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train exclusively on adversarial data\n",
    "adv_model.fit(perturbed_gen, epochs=3, validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:17:05.292177Z",
     "iopub.status.busy": "2025-04-15T16:17:05.291905Z",
     "iopub.status.idle": "2025-04-15T16:17:06.128136Z",
     "shell.execute_reply": "2025-04-15T16:17:06.127251Z",
     "shell.execute_reply.started": "2025-04-15T16:17:05.292157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weights_path_adv = '/kaggle/working/vgg19_adv_FGSMonly_cifake.weights.h5'\n",
    "model_path_adv   = '/kaggle/working/vgg19_adv_FGSMonly_cifake_model.h5'\n",
    "\n",
    "adv_model.save_weights(weights_path_adv)\n",
    "adv_model.save(model_path_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:08:14.958907Z",
     "iopub.status.busy": "2025-04-15T17:08:14.958175Z",
     "iopub.status.idle": "2025-04-15T17:10:24.823774Z",
     "shell.execute_reply": "2025-04-15T17:10:24.822892Z",
     "shell.execute_reply.started": "2025-04-15T17:08:14.958882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Accuracy: 0.5147\n",
      "FGSM Test Accuracy: 0.1875 (Baseline was ~12.5%)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on clean test data (optional)\n",
    "clean_loss, clean_acc = adv_model.evaluate(test_gen, verbose=0)\n",
    "print(f\"Clean Test Accuracy: {clean_acc:.4f}\")\n",
    "\n",
    "# Evaluate on FGSM-attacked test data\n",
    "x_test, y_test = next(test_gen)\n",
    "x_fgsm_test = fgsm_attack(adv_model, tf.convert_to_tensor(x_test), tf.convert_to_tensor(y_test), 0.07)\n",
    "fgsm_loss, fgsm_acc = adv_model.evaluate(x_fgsm_test, y_test, verbose=0)\n",
    "print(f\"FGSM Test Accuracy: {fgsm_acc:.4f} (Baseline was ~12.5%)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3041726,
     "sourceId": 5256696,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 301363,
     "modelInstanceId": 280459,
     "sourceId": 335042,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 304756,
     "modelInstanceId": 283914,
     "sourceId": 339508,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
